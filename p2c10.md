## 10. 深度解析NLP：循环神经网络（RNNs）

在第一章中，我们看到深度学习可用于自然语言数据集并取得卓越成果。我们的示例依赖于使用预训练语言模型并对其进行微调来分类评论。该示例突显了自然语言处理（NLP）与计算机视觉中迁移学习的差异：通常在NLP领域，预训练模型是在不同任务上训练的。

我们所说的语言模型，是一种经过训练后能够根据文本中已读内容预测下一个单词的模型。此类任务称为自监督学习（self-supervised learning）：我们无需为模型标注标签，只需输入海量文本数据。模型会自动从数据中获取标签，而这项任务绝非易事——要准确预测句子中的下一个单词，模型必须逐步建立对英语（或其他语言）的理解能力。自监督学习也可应用于其他领域；例如，参见 [《自监督学习与计算机视觉》](https://oreil.ly/ECjfJ) 了解视觉应用的入门知识。自监督学习通常不用于直接训练模型，而是用于预训练用于迁移学习的模型。

> 术语：自监督学习
>
> 使用嵌入在自变量中的标签训练模型，而非依赖外部标签。例如，训练模型预测文本中的下一个单词。

我们在第一章用于分类IMDb影评的语言模型是在维基百科上预训练的。通过直接将该语言模型微调为电影评论分类器，我们获得了很不错的结果，但如果增加一个步骤，效果将更佳。维基百科的英语与IMDb的英语存在细微差异，因此与其直接跳转到分类器，我们可先将预训练语言模型微调至IMDb语料库，再以此作为分类器的基础模型。

即使我们的语言模型掌握了任务所用语言的基础知识（例如预训练模型基于英语），适应目标语料库的风格仍大有裨益。该语料库可能采用更非正式的语言，或更具技术性，包含需要学习的新词汇或不同的句子构造方式。以IMDb数据集为例，其中包含大量电影导演和演员的名字，其语言风格通常比维基百科更不正式。

我们已经看到，借助fastai库，我们可以下载预训练的英语语言模型，并利用它在自然语言处理分类任务中获得最先进的结果。（我们预计更多语言的预训练模型很快就会面世；事实上，当你阅读本书时，它们很可能已经可用。）那么，我们为何还要深入学习如何训练语言模型呢？

当然，其中一个原因是理解所用模型的基础原理很有帮助。但还有另一个非常实际的原因：若在微调分类模型之前先对（基于序列的）语言模型进行微调，则能获得更优结果。例如在IMDb情感分析任务中，数据集包含50,000条未标注正负向标签的额外影评。由于训练集和验证集各含25,000条标注影评，总计构成100,000条影评数据。我们可以利用所有这些评论来微调预训练语言模型——该模型仅基于维基百科文章训练过；这将产生一个特别擅长预测电影评论下一个单词的语言模型。

这被称为通用语言模型微调（Universal Language Model Fine-tuning，ULMFiT）方法。该方法的论文表明，在将语言模型迁移学习到分类任务之前，额外添加这一微调阶段能显著提升预测效果。采用此方法后，我们在自然语言处理中的迁移学习包含三个阶段，如图10-1所示。

![10-1](img/10-1.jpg)

[^图10-1]: ULMFiT流程

接下来我们将探讨如何运用神经网络解决这个语言建模问题，并运用前两章介绍的概念。但在继续阅读之前，请先停下来思考一下：你会如何着手解决这个问题？

### 文本预处理

我们目前掌握的知识如何用于构建语言模型，这绝非显而易见。句子长度各异，文档篇幅冗长，那么如何利用神经网络预测句子的下一个词呢？让我们一起来看看吧！

我们已经了解了如何将分类变量用作神经网络的自变量。以下是我们对单个分类变量采用的方法：

1. 列出该分类变量所有可能的取值（我们将此列表称为词汇表（vocab））。
2. 将每个取值替换为其在词汇表中的索引。
3. 为其创建嵌入矩阵，其中每行对应一个取值（即词汇表中的每个项）。
4. 将该嵌入矩阵作为神经网络的第一层使用。（专用嵌入矩阵可直接接收步骤2生成的原始词表索引作为输入；这与接收表示索引的one-hot编码向量作为输入的矩阵功能等效，但前者速度更快且效率更高。）

我们也能对文本做几乎相同的事情！新颖之处在于序列的概念。首先我们将数据集中的所有文档拼接成一个超长字符串，并将其拆分为单词（或词元，tokens），从而得到一个超长的单词列表。我们的自变量将是由该超长列表中第一个单词开始、倒数第二个单词结束的单词序列，而因变量则是从第二个单词开始、到最后一个单词结束的单词序列。

我们的词汇表将包含两种词汇：预训练模型词汇库中已有的常用词，以及我们语料库中特有的新词（例如电影术语或演员姓名）。我们的嵌入矩阵将据此构建：对于预训练模型词汇表中的词汇，我们将取预训练模型嵌入矩阵中对应的行；但对于新词，由于没有对应项，我们将直接用随机向量初始化该行。

创建语言模型所需的每个步骤都涉及自然语言处理领域的专业术语，同时提供可用的fastai和PyTorch类库辅助实现。具体步骤如下：

- 分词处理

  将文本转换为单词列表（或字符列表、子字符串列表，具体取决于模型的粒度）。

- 数字化处理

  列出所有出现的唯一单词（词汇表），并通过查询词汇表中的索引将每个单词转换为数字。

- 语言模型数据加载器创建

  fastai提供的 `LMDataLoader` 类能自动处理创建与独立变量偏移一个令牌的依赖变量。它还可以处理若干关键细节，例如如何清洗训练数据以确保依赖变量与独立变量保持所需结构。

- 语言模型创建

  我们需要一种特殊模型来处理前所未有的任务：处理大小任意的输入列表。实现方式多种多样，本章将采用循环神经网络（RNN）。RNN的细节将在第12章详述，目前可将其视为另一种深度神经网络。

让我们详细看看每个步骤是如何运作的。

#### 词元化

当我们说“将文本转换为单词列表”时，遗漏了许多细节。例如，标点符号该如何处理？像“don't”这样的词该如何处理？它是一个词还是两个词？长长的医学术语或化学术语又该如何处理？是否应该将其拆解为独立的意义单元？连字符连接的词呢？德语和波兰语这类能由大量成分组合成超长单词的语言又该如何处理？而日语和汉语这类根本不使用词根、甚至没有明确词汇概念（译者注：汉语当然有词汇的概念，但是没有显式的词边界，也就是没有空格将句子分为若干个片段，且计算分词存在歧义）的语言呢？

由于这些问题没有唯一正确的答案，因此词元化也没有唯一的方法。主要有三种方法：

- 基于单词的分割方式

  将句子按空格分割，同时应用特定语言规则尝试分离语义单元（即使存在无空格情况，例如将“don't”拆分为“do n't”）。通常标点符号也会被分割为独立词素。

- 基于子词的分割方式

  将单词分割为更小的部分，基于最常出现的子字符串。例如，“occasion”可能被分割为“o c ca sion”。

- 基于字符的分割方式

  将句子分割为单个字符。

我们将在此探讨词级和次词级分词，而基于字符的分词则留待你在本章末尾的问卷中自行实现。

> 术语：词元
>
> 由分词过程创建的列表中的一个元素。它可以是一个单词、单词的一部分（一个子词）或一个字符。

#### 基于fastai的词元分割

fastai 并未提供专属的分词器，而是为外部库中的多种分词器提供统一接口。分词技术是活跃的研究领域，新型与改进型分词器层出不穷，因此 fastai 的默认分词器也会随之更新。但其 API 和选项不会发生太大变化，因为 fastai 致力于在底层技术迭代时保持 API 的一致性。

让我们用第一章中使用的IMDb数据集来试试看：

```python
from fastai.text.all import *
path = untar_data(URLs.IMDB)
```

我们需要获取文本文件以便测试分词器。正如 `get_image_files`（我们已多次使用该函数）能获取路径下的所有图像文件，`get_text_files` 则能获取路径下的所有文本文件。我们还可以选择性地传入 `folders` 参数，将搜索范围限定在特定子文件夹列表中：

```python
files = get_text_files(path, folders = ['train', 'test', 'unsup'])
```

以下是一篇评论，我们将对其进行分词处理（为节省篇幅，此处仅打印其开头部分）：

```python
txt = files[0].open().read(); txt[:75]
```

```text
'This movie, which I just discovered at the video store, has apparently sit '
```

在撰写本书时，fastai的默认英语词分词器使用了一个名为 `spaCy` 的库。它拥有一个复杂的规则引擎，包含针对URL、特定英语特殊词汇等场景的专属规则。不过我们不会直接使用 `SpacyTokenizer` ，而是采用 `WordTokenizer` ——该分词器始终指向fastai当前默认的词分词器（可能并非spaCy，具体取决于你使用的版本）。

让我们试试看。我们将使用 fastai 的 `coll_repr(collection, n)` 函数来显示结果。该函数会显示 `collection` 的前 `n` 个元素及其完整大小——这是 `L` 的默认实现方式。请注意，fastai 的分词器需要处理文档集合，因此我们需要将 `txt` 包裹在列表中：

```python
spacy = WordTokenizer()
toks = first(spacy([txt]))
print(coll_repr(toks, 30))
```

```text
(#201)
['This','movie',',','which','I','just','discovered','at','the','video','s
>
tore',',','has','apparently','sit','around','for','a','couple','of','years','
> without','a','distributor','.','It',"'s",'easy','to','see'...]
```

如你所见，spaCy主要只是将单词和标点符号分隔开来。但它在此还做了另一件事：将“it's”拆分为“it”和“'s”。这符合直觉——实际上它们本就是两个独立的词。当考虑到所有需要处理的细节时，词元化工作竟是如此微妙。好在spaCy能相当出色地处理这些情况——例如我们看到，当句号用于结束句子时会被拆分，但在缩写词或数字中则保持完整：

```python
first(spacy(['The U.S. dollar $1 is $1.00.']))
```

```text
(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']
```

fastai 随后通过 `Tokenizer` 类为分词过程添加了额外功能：

```python
tkn = Tokenizer(spacy)
print(coll_repr(tkn(txt), 31))
```

```text
(#228)
['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',
>
'the','video','store',',','has','apparently','sit','around','for','a','couple
>
','of','years','without','a','distributor','.','xxmaj','it',"'s",'easy'...]
```

请注意，现在出现了一些以字符“xx”开头的标记，这在英语中并非常见的词前缀。这些是特殊标记（special tokens）。

例如，列表中的第一个项目 `xxbos` 是一个特殊标记，表示新文本的开始（“BOS”是自然语言处理中的标准缩写，意为“流的开始”）。通过识别这个开始标记，模型将能够学会“忘记”先前内容，专注于后续词汇。

这些特殊标记并非直接来自spaCy。它们之所以存在，是因为fastai在处理文本时默认应用了若干规则来添加它们。这些规则旨在帮助模型更轻松地识别句子中的重要部分。某种程度上，我们正在将原始英语序列转换为一种简化的词元化语言——这种语言的设计初衷就是让模型更容易学习。

例如，规则会将四个感叹号序列替换为单个感叹号，后跟特殊重复字符标记，再接数字4。这样模型嵌入矩阵就能编码 *重复标点* （repeated character）等通用概念的信息，而无需为每种标点符号的重复次数单独设置标记。同样地，大写单词将被替换为特殊的大写标记，后跟该单词的小写版本。这样嵌入矩阵只需存储单词的小写形式，既节省计算和内存资源，又能学习到大写概念。

以下是一些你将看到的主要特殊标记：

- `xxbos`

  表示文本的开头（此处为评论）

- `xxmaj`

  表示下一个单词以大写字母开头（因为我们已将所有内容转换为小写）

- `xxunk`

  表示下一个单词未知

要查看使用的规则，你可以检查默认规则：

```python
defaults.text_proc_rules
```

```text
[<function fastai.text.core.fix_html(x)>,
<function fastai.text.core.replace_rep(t)>,
<function fastai.text.core.replace_wrep(t)>,
<function fastai.text.core.spec_add_spaces(t)>,
<function fastai.text.core.rm_useless_spaces(t)>,
<function fastai.text.core.replace_all_caps(t)>,
<function fastai.text.core.replace_maj(t)>,
<function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]
```

与往常一样，你可以在笔记本中输入下列内容查看每个示例的源代码：

```python
??replace_rep
```

以下是每项功能的简要说明：

- `fix_html`

  将特殊HTML字符替换为可读版本（IMDb评论中存在大量此类字符）

- `replace_rep`

  将重复三次及以上的任意字符替换为特殊重复标记（`xxrep`），后跟重复次数，最后是原始字符

- `replace_wrep`

  将重复三次及以上的单词替换为特殊标记：单词重复标记（`xxwrep`）+ 重复次数 + 原单词

- `spec_add_spaces`

  在 `/` 和 `#` 周围添加空格

- `rm_useless_spaces`

  移除所有重复出现的空格字符

- `replace_all_caps`

  将全大写单词转换为小写，并在其前添加全大写标记（`xxcap`）

- `replace_maj`

  将首字母大写的单词转换为小写，并在其前添加首字母大写标记（`xxmaj`）

- `lowercase`

  将所有文本转换为小写，并在开头添加小写标记（`xxbos`）和/或结尾添加全小写标记（`xxeos`）

让我们看看其中几个的实际应用：

```python
coll_repr(tkn('&copy; Fast.ai www.fast.ai/INDEX'), 31)
```

```text
"(#11)
['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','ind
> ex'...]"
```

现在让我们看看子词分词器是如何工作的。

#### 子词分词器

除了前文所述的词分隔方法外，另一种流行的分隔方式是子词分隔。词分隔基于这样一个假设：空格能有效区分句子中各语义成分。然而这种假设并不总是成立。例如考虑这句话：我的名字是郝杰瑞（中文对应英文“My name is Jeremy Howard”）。这种情况用词分词器处理效果很差，因为句子中根本没有空格！中文和日语等语言不使用空格，实际上它们甚至没有明确的“词”概念。而土耳其语和匈牙利语等语言可以将多个子词无空格地组合在一起，形成包含大量独立信息的长词。

处理这些情况时，通常最好采用子词分词法。该方法分为两个步骤：

1. 分析文档语料库，找出最常出现的字母组合。这些组合即构成词汇表。
2. 使用该词汇表中的子词单元对语料库进行分词处理。

让我们看一个例子。对于我们的语料库，我们将使用前2000条电影评论：

```python
txts = L(o.open().read() for o in files[:2000])
```

我们实例化分词器时，需传入要创建的词表大小，随后需要对其进行“训练”。即让其读取文档并找出常见字符序列以构建词表。此操作通过 `setup` 方法实现。稍后将看到，`setup` 是fastai的特殊方法，在常规数据处理流程中会自动调用。但当前我们手动操作所有步骤，因此需自行调用该方法。以下函数根据指定词表大小执行这些步骤，并展示示例输出：

```py
def subword(sz):
    sp = SubwordTokenizer(vocab_sz=sz)
    sp.setup(txts)
    return ' '.join(first(sp([txt]))[:40])
```

让我们试试看：

```python
subword(1000)
```

```text
'▁This ▁movie , ▁which ▁I ▁just ▁dis c over ed ▁at ▁the ▁video ▁st
or e , ▁has
> ▁a p par ent ly ▁s it ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁dis t
> ri but or . ▁It'
```

使用 fastai 的子词分词器时，特殊字符 `_` 代表原始文本中的空格字符。

如果使用较小的词汇表，每个词元将代表更少的字符，因此需要更多词元来表示一个句子：

```python
subword(200)
```

```text
'▁ T h i s ▁movie , ▁w h i ch ▁I ▁ j us t ▁ d i s c o ver ed ▁a t ▁the ▁ v id e
> o ▁ st or e , ▁h a s'
```

另一方面，如果我们使用更大的词汇表，大多数常见的英语单词最终会进入词汇表本身，这样我们就不需要用那么多单词来表示一个句子：

```python
subword(10000)
```

```text
"▁This ▁movie , ▁which ▁I ▁just ▁discover ed ▁at ▁the ▁video ▁store
, ▁has
> ▁apparently ▁sit ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁distributor
> . ▁It ' s ▁easy ▁to ▁see ▁why . ▁The ▁story ▁of ▁two ▁friends ▁living"
```

选择子词词汇量大小是一种权衡：更大的词汇量意味着每句的词元数量减少，从而加快训练速度、降低内存需求，并减少模型需要记忆的状态；但另一方面，这意味着更大的嵌入矩阵，需要更多数据来学习。

总体而言，子词分词技术提供了一种在字符分词（即使用小型子词词库）与词分词（即使用大型子词词库）之间轻松扩展的方法，且能处理所有人类语言而无需开发特定语言算法。它甚至能处理其他“语言”，例如基因组序列或MIDI乐谱！正因如此，过去一年间该方法的普及度急剧攀升，极有可能成为最主流的分词方案（待您阅读本文时，它或许已然如此！）。

将文本分割为词元后，我们需要将其转换为数字。接下来我们将探讨这一过程。

#### 使用fastai进行数值化

数值化（Numericalization）是将标记映射为整数的处理过程。其步骤与创建 `Category` 变量基本相同，例如MNIST数据集中的数字依赖变量：

1. 列出该分类变量（词表）的所有可能取值（即词表）。
2. 将每个取值替换为其在词表中的索引。

让我们看看它在之前看到的词分隔文本中的实际应用：

```python
toks = tkn(txt)
print(coll_repr(tkn(txt), 31))
```

```text
(#228)
['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',
>
'the','video','store',',','has','apparently','sit','around','for','a','couple
>
','of','years','without','a','distributor','.','xxmaj','it',"'s",'easy'...]
```

与 `SubwordTokenizer` 类似，我们需要调用 `Numericalize` 的 `setup` 方法；这是创建词汇表的方式。这意味着我们首先需要已分词的语料库。由于分词过程耗时较长，fastai会并行处理；但在此手动演示中，我们将使用一个小型子集：

```python
toks200 = txts[:200].map(tkn)
toks200[0]
```

```text
(#228)
>
['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at'...]
```

我们可以将此传递给 `setup` 来创建词汇表：

```python
num = Numericalize()
num.setup(toks200)
coll_repr(num.vocab,20)
```

```text
"(#2000)
['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj
> ','the','.',',','a','and','of','to','is','in','i','it'...]"
```

我们的特殊规则标记优先出现，随后每个词按出现频率排序各出现一次。`Numericalize` 的默认参数为 `min_freq=3` 和 `max_vocab=60000` 。当 `max_vocab=60000` 时，fastai会将除最常见的60,000个词之外的所有词替换为特殊未知词标记 `xxunk`。此机制有助于避免嵌入矩阵过度膨胀——过大的矩阵会拖慢训练速度、消耗过多内存，且可能导致罕见词缺乏足够训练数据来构建有效表示。不过后者问题更宜通过调整 `min_freq` 解决：默认值 `3` 意味着出现次数少于3次的词汇均会被替换为 `xxunk`。

fastai 还可通过你提供的词汇表对数据集进行数值化处理，只需将词汇列表作为 `vocab` 参数传递即可。

创建 `Numericalize` 对象后，我们就可以像使用函数一样使用它。

```python
nums = num(toks)[:20]; nums
```

```text
tensor([ 2, 8, 21, 28, 11, 90, 18, 59, 0, 45, 9, 351, 499,
11,
> 72, 533, 584, 146, 29, 12])
```

这次，我们的词元已被转换为模型可接收的整数张量。我们可以验证它们映射回原始文本：

```python
' '.join(num.vocab[o] for o in nums)
```

```text
'xxbos xxmaj this movie , which i just xxunk at the video store , has
apparently
> sit around for a'
```

既然我们已经有了数据，就需要将它们分批处理以供模型使用。

#### 将文本分批处理以供语言模型使用

在处理图像时，我们需要先将所有图像调整为相同高度和宽度，再将其分组为小批量数据，以便高效地堆叠到单个张量中。而文字处理则有所不同，因为文字无法简单地调整为指定长度。此外，我们希望语言模型按顺序读取文本，从而高效预测下一个单词。这意味着每个新批次必须精确接续上一个批次结束的位置。

假设我们有以下文本：

> In this chapter, we will go back over the example of classifying movie reviews we
> studied in chapter 1 and dig deeper under the surface. First we will look at the
> processing steps necessary to convert text into numbers and how to customize it. By doing this, we’ll have another example of the PreProcessor used in the data block API.
> Then we will study how we build a language model and train it for a while.

词元化过程将添加特殊标记并处理标点符号，最终返回以下文本：

> xxbos xxmaj in this chapter , we will go back over the example of classifying
> movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj
> first we will look at the processing steps necessary to convert text into numbers
> and how to customize it . xxmaj by doing this , we ‘ll have another example of the preprocessor used in the data block xxup api . \n xxmaj then we will study how we build a language model and train it for a while .

我们现在有90个词元，它们之间用空格分隔。假设我们想要批处理大小为6。我们需要将这段文本拆分为6个连续的部分，每个部分长度为15：

![p531](img/p531.png)

在理想情况下，我们可以将这批数据直接输入模型。但这种方法无法扩展，因为在实际应用中，很难将包含所有标记的单批数据装入GPU内存（此处仅90个词元，而IMDb所有评论合计达数百万条）。

因此，我们需要将这个数组更精细地划分为具有固定序列长度的子数组。保持这些子数组内部及跨子数组的顺序至关重要，因为我们将使用一种维护状态的模型——该模型在预测后续内容时，能够记住先前读取的内容。

回到我们之前那个包含6个长度为15的批次的例子，如果我们选择序列长度为5，这意味着我们首先输入以下数组：

![p532](img/p532.png)

然后是这个：

![p532.2](img/p532.2.png)

最后：

![p533](img/p533.png)

回到我们的电影评论数据集，第一步是将单个文本拼接成一个流。与图像处理类似，最佳做法是随机化输入顺序。因此在每个训练迭代轮次的开始，我们将打乱条目顺序生成新数据流（注意：打乱的是文档顺序而非文档内单词顺序，否则文本将失去语义！）。

然后我们将该数据流分割为若干批次（即批次大小）。

例如，若数据流包含50,000个词元且批次大小设为10，则会生成10个各含5,000个词元的子数据流。关键在于我们保留了词元的顺序（即第一个子流从1到5000，第二个从5001到10000...），因为我们希望模型能够连续读取文本行（如前例所示）。预处理阶段会在每段文本开头添加 `xxbos` 标记，以便模型在读取流数据时识别新条目的起始位置。

简而言之，在每个时间步长中，我们将文档集合重新洗牌并串联成一个词元流。随后将该流切割成若干固定长度的连续子流。模型会按顺序读取这些子流，并借助内部状态机制，无论选择何种序列长度，都能产生相同的激活结果。

当我们创建一个 `LMDataLoader` 时，这些操作都是由fastai库在后台自动完成的。具体实现方式是：首先将我们的 `Numericalize` 对象应用于分词后的文本数据。

```python
nums200 = toks200.map(num)
```

然后将它传递给 `LMDataLoader`：

```python
dl = LMDataLoader(nums200)
```

让我们通过获取第一批数据来确认这是否产生了预期结果。

```python
x,y = first(dl)
x.shape,y.shape
```

```text
(torch.Size([64, 72]), torch.Size([64, 72]))
```

然后查看自变量的第一行，它应该是第一个文本的开头：

```python
' '.join(num.vocab[o] for o in x[0][:20])
```

```text
'xxbos xxmaj this movie , which i just xxunk at the video store , has
apparently
> sit around for a'
```

因变量是相同项偏移一个词元的结果：

```python
' '.join(num.vocab[o] for o in y[0][:20])
```

```text
'xxmaj this movie , which i just xxunk at the video store , has
apparently sit
> around for a couple'
```

至此，我们对数据所需的所有预处理步骤均已完成。现在，我们已准备好训练文本分类器。

### 训练文本分类器

正如本章开头所述，使用迁移学习训练尖端文本分类器需要两个步骤：首先，我们需要将预先在维基百科上训练的语言模型微调到IMDb评论语料库上；然后，我们可以使用该模型来训练分类器。

老规矩，我们先从整理数据开始。

#### 基于DataBlock的语言模型

当 `TextBlock` 被传递给 `DataBlock` 时，fastai会自动处理分词和数值化操作。所有可传递给 `Tokenizer` 和 `Numericalize` 的参数同样可传递给 `TextBlock` 。在下一章中，我们将讨论分别执行这些步骤的最简便方法，以简化调试过程。但你始终可以参照前文所述，通过在数据子集上手动运行这些步骤进行调试。别忘了 `DataBlock` 的便捷 `summary` 方法，它对排查数据问题非常有用。

以下是使用 `TextBlock` 创建语言模型的方法，采用 fastai 的默认设置：

```python
get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])

dls_lm = DataBlock(
    blocks=TextBlock.from_folder(path, is_lm=True),
    get_items=get_imdb, splitter=RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
```

与我们在 `DataBlock` 中使用过的其他类型不同的是，我们并非直接调用类本身（例如 `TextBlock(...)` ），而是调用类方法。类方法是Python中属于类而非对象的方法（若不熟悉此概念，请务必在线搜索相关资料，因为它在众多Python库和应用中被广泛使用；本书此前虽多次用到，但未特别强调）。`TextBlock` 的特殊性在于：数值化器的词汇表构建过程可能耗时较长（我们需要读取并分词每份文档才能生成词表）。

为了尽可能提高效率，fastai 进行了若干优化：

- 它将分词后的文档保存在临时文件夹中，因此无需重复进行分词处理。
- 它通过并行运行多个分词进程，充分利用计算机的CPU资源。

我们需要告诉 `TextBlock` 如何访问文本，以便它能执行这项初始预处理——这就是 `from_folder` 的作用。

`show_batch` 随后按常规方式运行：

```python
dls_lm.show_batch(max_n=2)
```

| text                                                         | text_                                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| xxbos xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it ’s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk |
| what xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i ‘ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . \n\n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is |

现在数据准备就绪，我们可以对预训练语言模型进行微调。

#### 语言模型的微调

要将整数词索引转换为可用于神经网络的激活值，我们将采用嵌入技术——正如在协同过滤和表格建模中使用的那样。随后，我们将这些嵌入值输入到循环神经网络中，采用名为AWD-LSTM的架构（第12章将展示如何从零构建此类模型）。如前所述，预训练模型中的嵌入向量会与随机嵌入向量合并，后者用于补充预训练词汇表中缺失的词汇。此过程在 `language_model_learner` 内部自动完成：

```python
learn = language_model_learner(
    dls_lm, AWD_LSTM, drop_mult=0.3,
    metrics=[accuracy, Perplexity()]).to_fp16()
```

默认使用的损失函数是交叉熵损失，因为我们本质上面临的是分类问题（不同类别即词汇表中的单词）。此处采用的困惑度度量常用于自然语言处理中的语言模型：它是损失值的指数（即 `torch.exp(cross_entropy)`）。我们同时引入准确率指标，用于衡量模型预测下一个单词时的正确次数——因为交叉熵（如前所述）既难以解释，又更侧重反映模型置信度而非实际准确性。

让我们回到本章开头的流程图。第一阶段的箭头已为我们完成，并作为预训练模型在fastai中提供，而我们刚刚为第二阶段构建了 `DataLoaders` 和 `Learner` 。现在，我们准备好对语言模型进行微调了！

![p537](img/p537.jpg)

每次训练一个迭代轮次需要相当长的时间，因此我们将在训练过程中保存中间模型结果。由于 `fine_tune` 不会自动执行此操作，我们将使用 `fit_one_cycle`。与 `cnn_learner` 类似，`language_model_learner` 在使用预训练模型时（默认情况）会自动调用 `freeze`，因此仅训练嵌入层（模型中唯一包含随机初始化权重的部分——即 IMDb 词汇表中存在但预训练模型词汇表中缺失的词汇嵌入）：

```python
learn.fit_one_cycle(1, 2e-2)
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 困惑度    | 时间  |
| -------- | -------- | -------- | -------- | --------- | ----- |
| 0        | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 |

该模型训练耗时较长，因此正好可以借此机会讨论保存中间结果的问题。

#### 模型的保存与加载

你可以像这样轻松地保存模型的状态：

```python
learn.save('1epoch')
```

这将在 `learn.path/models/` 目录下创建名为 `1epoch.pth` 的文件。如果你希望在另一台机器上以相同方式创建学习器后加载模型，或稍后继续训练，可按以下方式加载该文件内容：

```python
learn = learn.load('1epoch')
```

初始训练完成后，我们可以继续进行模型解冻后的微调：

```python
learn.unfreeze()
learn.fit_one_cycle(10, 2e-3)
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 困惑度    | 时间  |
| -------- | -------- | -------- | -------- | --------- | ----- |
| 0        | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 |
| 1        | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 |
| 2        | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 |
| 3        | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 |
| 4        | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 |
| 5        | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 |
| 6        | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 |
| 7        | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 |
| 8        | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 |
| 9        | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 |

完成上述操作后，我们将保存模型除最终层之外的所有部分——该最终层负责将激活值转换为词汇表中每个词元的选择概率。不含最终层的模型称为编码器（encoder），可通过`save_encoder` 函数保存：

```python
learn.save_encoder('finetuned')
```

> 术语：编码器
>
> 该模型不包含任务特异性最终层。该术语在视觉卷积神经网络中的含义与“主体”基本相同，但在自然语言处理和生成模型中，“编码器”一词更常用。

至此，文本分类流程的第二阶段——语言模型的微调工作已完成。现在我们可以利用该模型，结合IMDb情感标签对分类器进行微调。不过在进入分类器微调环节之前，让我们先尝试一个不同的操作：用我们的模型生成随机影评。

#### 生成文本

由于我们的模型经过训练能够预测句子中的下一个单词，因此我们可以利用它来编写新的评论：

```python
TEXT = "I liked this movie because"
N_WORDS = 40
N_SENTENCES = 2
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
         for _ in range(N_SENTENCES)]
```

```python
print("\n".join(preds))
```

```text
i liked this movie because of its story and characters . The story line
was very
> strong , very good for a sci - fi film . The main character , Alucard
, was
> very well developed and brought the whole story
i liked this movie because i like the idea of the premise of the movie ,
the (
> very ) convenient virus ( which , when you have to kill a few people ,
the "
> evil " machine has to be used to protect
```

如你所见，我们加入了随机性（根据模型返回的概率随机选择单词），因此不会出现完全相同的评论。我们的模型并未预先编程任何句式结构或语法规则，却明显掌握了大量英语句式特征：可见其正确使用大写字母（“I”被转换为小写“i”，因规则要求单词需两个字符以上才视为大写，故小写处理属正常现象），且时态运用保持一致。这篇综合评论乍看合乎逻辑，只有仔细阅读才能察觉某些细节略有偏差。对于仅训练数小时的模型而言，这已相当出色！

但我们的最终目标并非训练模型生成评论，而是对评论进行分类……那么就让我们用这个模型来实现这个目标吧。

#### 创建分类器数据加载器

我们现在正从语言模型微调转向分类器微调。回顾一下，语言模型预测文档的下一个单词，因此不需要任何外部标签。而分类器则预测外部标签——以IMDb为例，它预测的是文档的情感倾向。

这意味着我们用于自然语言处理分类的 `DataBlock` 结构将看起来非常熟悉。它几乎与我们处理过的众多图像分类数据集完全相同：

```python
dls_clas = DataBlock(
    blocks=(TextBlock.from_folder(path,
                                  vocab=dls_lm.vocab),CategoryBlock),
    get_y = parent_label,
    get_items=partial(get_text_files, folders=['train', 'test']),
    splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path, path=path, bs=128, seq_len=72)
```

与图像分类类似，`show_batch` 方法会展示每个自变量（本例中为电影评论文本）对应的因变量（即情感倾向）：

```python
dls_clas.show_batch(max_n=3)
```

| 序号 | 文本                                                         | 类别 |
| ---- | ------------------------------------------------------------ | ---- |
| 0    | xxbos i rate this movie with 3 skulls , only coz the girls knew how to scream , this could ‘ve been a better movie , if actors were better , the twins were xxup ok , i believed they were evil , but the eldest and youngest brother , they sucked really bad , it seemed like they were reading the scripts instead of acting them … . spoiler : if they ‘re vampire ’s why do they freeze the blood ? vampires ca n’t drink frozen blood , the sister in the movie says let ’s drink her while she is alive … .but then when they ‘re moving to another house , they take on a cooler they ‘re frozen blood . end of spoiler \n\n it was a huge waste of time , and that made me mad coz i read all the reviews of how | neg  |
| 1    | xxbos i have read all of the xxmaj love xxmaj come xxmaj softly books . xxmaj knowing full well that movies can not use all aspects of the book , but generally they at least have the main point of the book . i was highly disappointed in this movie . xxmaj the only thing that they have in this movie that is in the book is that xxmaj missy ’s father comes to xxunk in the book both parents come ) . xxmaj that is all . xxmaj the story line was so twisted and far fetch and yes , sad , from the book , that i just could n’t enjoy it . xxmaj even if i did n’t read the book it was too sad . i do know that xxmaj pioneer life was rough , but the whole movie was a downer . xxmaj the rating | neg  |
| 2    | xxbos xxmaj this , for lack of a better term , movie is lousy . xxmaj where do i start … … \n\n xxmaj cinemaphotography - xxmaj this was , perhaps , the worst xxmaj i ‘ve seen this year . xxmaj it looked like the camera was being tossed from camera man to camera man . xxmaj maybe they only had one camera . xxmaj it gives you the sensation of being a volleyball . \n\n xxmaj there are a bunch of scenes , haphazardly , thrown in with no continuity at all . xxmaj when they did the ' split screen ' , it was absurd . xxmaj everything was squished flat , it looked ridiculous . \n\n xxmaj the color tones were way off . xxmaj these people need to learn how to balance a camera . xxmaj this ' movie ' is poorly made , and | neg  |

来看看 `DataBlock` 的定义，每个部分都与先前构建的数据块相似，但存在两个重要例外：

- `TextBlock.from_folder` 不再包含 `is_lm=True` 参数。
- 我们将创建的 `vocab` 传递给语言模型微调。

我们传递语言模型的词汇表是为了确保使用相同的词元到索引映射关系。否则，我们在微调语言模型中学习到的嵌入向量将无法被该模型理解，微调步骤也将毫无意义。

通过传递 `is_lm=False`（或完全不传递 `is_lm`，因为其默认值为 `False`），我们告知 `TextBlock` 当前处理的是常规标注数据，而非将后续词元作为标签使用。然而我们仍需应对一个挑战，即如何将多份文档整合为一个小批次。让我们通过一个示例来演示：尝试创建包含前10个文档的小批次。首先我们将它们数值化：

```python
nums_samp = toks200[:10].map(num)
```

现在让我们看看这10篇电影评论中每篇包含多少个词元：

```python
nums_samp.map(len)
```

```text
(#10) [228,238,121,290,196,194,533,124,581,155]
```

请记住，PyTorch `DataLoaders` 需要将批次中的所有项目整合为单个张量，而单个张量具有固定形状（即每个轴上都有特定长度，且所有项目必须保持一致）。这应该听起来很熟悉：我们在处理图像时也遇到过相同的问题。当时我们通过裁剪、填充和/或压缩手段使所有输入达到相同尺寸。但裁剪对文档可能并非良策，因为很可能导致关键信息丢失（尽管图像同样存在此问题，我们仍采用裁剪；目前自然语言处理领域尚未充分探索数据增强技术，或许裁剪在NLP中同样存在应用空间！）。文档无法进行“压缩”操作，因此只剩下填充这一方案！

我们将扩展最短文本使其长度一致。为此，我们使用一种特殊填充词元，该词元会被模型忽略。此外，为避免内存问题并提升性能，我们将把长度大致相同的文本批量处理（训练集会进行一定程度的打乱）。具体实现方式是：在每个训练周期开始前，大致按文档长度对训练集进行排序。这样整理成单批次的文档长度就会趋于一致。我们不会将每个批次都填充到相同大小，而是以批次中最长文档的长度作为目标尺寸。

> 动态调整图像大小
>
> 对图像进行类似操作也是可行的，这对于尺寸不规则的矩形图像尤为有用。但在撰写本文时，尚无库对此提供良好支持，也没有相关论文探讨该问题。不过我们计划近期将此功能添加到fastai中，敬请关注本书官网；一旦功能完善，我们将立即发布相关信息。

当使用 `is_lm=False` 的 `TextBlock` 时，数据块 API 会自动为我们完成排序和填充操作。（对于语言模型数据，我们不会遇到同样的问题，因为我们会先将所有文档拼接在一起，然后再将其分割为等长段落。）

现在我们可以创建一个模型来对文本进行分类：

```python
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,
                                metrics=accuracy).to_fp16()
```

在训练分类器之前，最后一步是从我们微调过的语言模型中加载编码器。我们使用 `load_encoder` 而非 `load` ，因为编码器仅有预训练权重可用；若加载不完整模型， `load` 默认会引发异常：

```python
learn = learn.load_encoder('finetuned')
```

#### 分类器的微调

最后一步是采用鉴别学习率和渐进解冻策略（gradual unfreezing）进行训练。在计算机视觉领域，我们通常一次性解冻整个模型，但对于自然语言处理分类器，我们发现每次解冻少量层级能带来显著差异：

```python
learn.fit_one_cycle(1, 2e-2)
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 时间  |
| -------- | -------- | -------- | -------- | ----- |
| 0        | 0.347427 | 0.184480 | 0.929320 | 00:33 |

仅用一个迭代轮次，我们就得到了与第1章训练相同的成果——还不错！我们可以向 `freeze_to` 传递 `-2` 参数，以此冻结除最后两个参数组之外的所有参数：

```python
learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 时间  |
| -------- | -------- | -------- | -------- | ----- |
| 0        | 0.247763 | 0.171683 | 0.934640 | 00:37 |

然后我们可以再稍稍解冻一下，继续训练：

```python
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 时间  |
| -------- | -------- | -------- | -------- | ----- |
| 0        | 0.193377 | 0.156696 | 0.941200 | 00:45 |

最后，我们来解冻整个模型！

```python
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
```

| 迭代轮次 | 训练损失 | 验证损失 | 准确率   | 时间  |
| -------- | -------- | -------- | -------- | ----- |
| 0        | 0.172888 | 0.153770 | 0.943120 | 01:01 |
| 1        | 0.161492 | 0.155567 | 0.942640 | 00:57 |

我们实现了94.3%的准确率，这在三年前还是顶尖水平。通过用所有倒序文本训练另一个模型，并取两个模型的预测结果平均值，我们甚至能达到95.1%的准确率——这正是ULMFiT论文提出的最新纪录。该纪录仅在数月前被打破——通过对更大规模模型进行微调，并采用昂贵的数据增强技术（将句子翻译成另一种语言再译回，同时使用另一模型进行翻译）。

利用预训练模型，我们可以构建出功能强大的微调语言模型，既能生成虚假评论，也能协助识别虚假评论。这项技术令人振奋，但需谨记它同样可能被用于恶意目的。

### 虚假信息与语言模型

在深度学习语言模型尚未普及的年代，即便基于规则的简单算法也能被用于创建虚假账户，试图影响政策制定者。现任ProPublica计算记者的杰夫·高（Jeff Kao）分析了2017年美国联邦通信委员会（FCC）关于废除网络中立性提案的公众评论。他在题为 [《百万份废除网络中立性支持意见或系伪造》](https://oreil.ly/ptq8B) 中，他揭露了大量反对网络中立性的评论疑似通过某种填空式邮件合并程序生成。图10-2中，高将虚假评论进行色彩标注，清晰展现其公式化特征。

![10-2](img/10-2.jpg)

[^图10-2]: 美国联邦通信委员会在网络中立性辩论期间收到的评论

高估算称，“在超过2200万条评论中，真正独特的不足80万条”，且“超过99%的真正独特评论都支持保留网络中立性”。

鉴于2017年以来语言建模技术的进步，如今此类欺诈活动几乎难以察觉。你现在已掌握所有必要工具，足以创建令人信服的语言模型——这种模型能生成符合语境且可信的文本。它未必完全准确无误，但绝对具有说服力。试想这项技术若与近年我们所知的虚假信息传播活动相结合会产生何种影响。请看图10-3展示的Reddit对话：基于OpenAI GPT-2算法的语言模型正就“美国政府是否应削减国防开支”这一议题进行自我对话。

![10-3](img/10-3.jpg)

[^图10-3]: Reddit上自我对话的算法

在此案例中，对话是由算法生成的。但试想若恶意行为者决定在社交网络中释放此类算法——他们可缓慢谨慎地操作，让算法随时间推移逐步积累追随者与信任度。要操控数百万账号实施此类操作，所需资源其实并不多。在这种情境下，我们不难想象最终会出现这样的局面：网络上绝大多数言论都来自机器人账号，而无人察觉其中玄机。

我们已经开始看到机器学习被用于生成身份的实例。例如，图10-4展示了凯蒂·琼斯（Katie Jones）的领英个人资料。

![10-4](img/10-4.jpg)

[^图10-4]: 凯蒂·琼斯（Katie Jones）的领英个人资料

凯蒂·琼斯在领英上与华盛顿多家主流智库成员互相关联。但她根本不存在。你所看到的这张照片是由生成对抗网络自动生成的，事实上，根本不存在名为凯蒂·琼斯的人从战略与国际研究中心毕业。

许多人认为或希望算法能在此为我们提供保护——即我们能开发出能够自动识别自动生成内容的分类算法。然而问题在于，这永远是一场军备竞赛：更优的分类（或鉴别器）算法反而会被用来创造更强大的生成算法。

### 总结

在本章中，我们探讨了fastai库最后一个开箱即用的应用领域：文本。我们接触了两种模型：能够生成文本的语言模型，以及判定评论正负向的分类器。为构建前沿分类器，我们采用预训练语言模型，针对任务语料集进行微调，再将其主体（编码器）与新头部结合进行分类任务。

在本书这一部分结束之前，我们将探讨fastai库如何帮助你针对具体问题整合数据。

### 问卷调查

1. 什么是自监督学习？
2. 什么是语言模型？
3. 为什么语言模型被视为自监督模型？
4. 自监督模型通常用于什么？
5. 为什么要对语言模型进行微调？
6. 创建尖端文本分类器的三个步骤是什么？
7. 50,000条无标签电影评论如何帮助创建更优的IMDb数据集文本分类器？

8. 为语言模型准备数据需要哪三个步骤？
9. 什么是词元化？为何我们需要这样做？
10. 请列举三种词元化的方法。
11. 什么是 `xxbos` ？
12. 请列举fastai在文本分词过程中应用的四条规则。
13. 为何重复字符会被替换为显示重复次数及重复字符的标记？
14. 什么是数字化处理？
15. 为何某些词会被替换为“未知词”标记？
16. 当批量大小为64时，表示首个批次的张量首行包含数据集前64个词元。该张量的第二行包含什么？第二个批次的首行又包含什么？（注意——学生常答错此题！请务必在教材官网核对答案。）
17. 为何文本分类需要填充？语言模型为何不需要填充？
18. NLP的嵌入矩阵包含哪些内容？其形状如何？
19. 什么是困惑度？
20. 为何必须将语言模型的词汇表传递给分类器数据块？
21. 什么是渐进解冻？
22. 为何文本生成技术往往领先于机器生成文本的自动识别技术？

#### 进一步研究

1. 了解语言模型与虚假信息的相关知识。当前最优秀的语言模型有哪些？观察它们的输出结果。你认为它们具有说服力吗？恶意行为者如何利用此类模型制造冲突与不确定性？

2. 鉴于模型难以持续识别机器生成的文本，面对利用深度学习的大规模虚假信息传播活动，我们还需要哪些其他应对策略？
